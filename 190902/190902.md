### Spark

#### 6.4

##### DataFrameWriter와 DataFrameReader로 구조화 데이터 read/write 실습

```SPARQL
case class Dessert(menuId:String, name:String, price: Int, kcal:Int)
val dessertRDD = sc.textFile("/data/spark/dessert-menu.csv")
val dessertDF = dessertRDD.map{ record =>
	val splitRecord = record.split(",")
	val menuId = splitRecord(0)
	val name = splitRecord(1)
	val price = splitRecord(2).toInt
	val kcal = splitRecord(3).toInt
	Dessert(menuId, name, price, kcal)
}.toDF

val dfWriter = dessertDF.write
dfWriter.format("parquet").save("/output/spark/dessert-parquet")

val dfReader = spark.read
val dessertDF2 = dfReader.format("parquet").load("/output/spark/dessert-parquet")
dessertDF2.orderBy($"name").show(3)
```

###### 결과 > 

```bash
scala> dessertDF2.orderBy($"name").show(3)
+------+-------------+-----+----+
|menuId|         name|price|kcal|
+------+-------------+-----+----+
|  D-11|고구마 파르페| 6500| 650|
|  D-12|    녹차 빙수| 3800| 320|
|   D-7|  녹차 파르페| 4500| 380|
+------+-------------+-----+----+
only showing top 3 rows
```





##### 테이블형식으로 저장, 조회

```SPARQL
dessertDF.write.format("parquet").saveAsTable("dessert_tbl_parquet")
spark.read.format("parquet").table("dessert_tbl_parquet").show(3)
```

###### 결과 >

```bash
scala> spark.read.format("parquet").table("dessert_tbl_parquet").show(3)
+------+-------------+-----+----+
|menuId|         name|price|kcal|
+------+-------------+-----+----+
|   D-0|초콜릿 파르페| 4900| 420|
|   D-1|  푸딩 파르페| 5300| 380|
|   D-2|  딸기 파르페| 5200| 320|
+------+-------------+-----+----+
only showing top 3 rows

#spark.sql("select * from dessert_tbl_parquet limit 3").show 도 가능

```



##### 세이프 모드

출력 장소에 데이터셋이 이미 존재할 경우 어떻게 처리할지를 결정

- SaveModeErrorIfExosts - 예외를 발생시킨다(디폴트)
- SaveModeAppend - 기존 데이터셋에 덧붙인다
- SaveMode.Overwite - 기존 데이터셋을 덮어쓴다
- SaveMode.Ignore - 기존 데이터셋을 변경하지 않는다

```bash
# hadoop fs -mkdir /output/dessert_json 으로 만든 후 
dessertDF.write.save("/output/dessert_json") # < 에러 발생
```

```bash
# 에러 X
import org.apache.spark.sql.SaveMode
dessertDF.write.format("json").mode(SaveMode.Overwrite).save("/output/dessert_json")
```



##### 명시적으로 스키마 정보 부여



```bash
import java.math.BigDecimal
case class DecimalTypeContainer(data:BigDecimal)
val bdContainerDF = sc.parallelize(
	List(new BigDecimal("12345.67899999999999"))
	).map(data => DecimalTypeContainer(data)).toDF

scala> bdContainerDF.printSchema
 root
 |-- data: decimal(38,18) (nullable = true)
scala> bdContainerDF.show(false)
+--------------------+
|data                |
+--------------------+
|12345.67899999999999|
+--------------------+
```

```bash
bdContainerDF.write.format("orc").save("/output/spark/bdContainerORC")
val bdContainerORCDF = spark.read.format("orc").load("/output/spark/bdContainerORC")

scala> bdContainerORCDF.printSchema
root
 |-- data: decimal(38,18) (nullable = true)
scala> bdContainerORCDF.show(false)
+------------------------+
|data                    |
+------------------------+
|12345.678999999999990000|
+------------------------+

```

```bash
bdContainerDF.write.format("json").save("/output/bdContainerJSON")
val bdContainerJSONDF = spark.read.format("json").load("/output/bdContainerJSON")
bdContainerJSONDF.printSchema
bdContainerJSONDF.show(false)


scala> bdContainerJSONDF.printSchema
root
 |-- data: double (nullable = true)


scala> bdContainerJSONDF.show(false)
+---------+
|data     |
+---------+
|12345.679|
+---------+

```

```bash
import org.apache.spark.sql.types.DataTypes._
val schema = createStructType(Array(createStructField("data", createDecimalType(38, 18), true)))

```

```bash
val bdContainerJSONDF = spark.read.schema(schema).format("json").load("/output/bdContainerJSON")
bdContainerJSONDF.printSchema
bdContainerJSONDF.show(false)

scala> bdContainerJSONDF.printSchema
root
 |-- data: decimal(38,18) (nullable = true)


scala> bdContainerJSONDF.show(false)
+------------------------+                                                      
|data                    |
+------------------------+
|12345.678999999999990000|
+------------------------+
```



##### 파티셔닝

DataFrameWriter의 partitionBy 메서드를 이용하면 DataFrame이 나타내는 데이터셋을 파티셔닝해서 출력할 수 있다.

```bash
import org.apache.spark.sql.types.DataTypes._
val priceRangeDessertDF = dessertDF.select(((($"price" / 1000) cast IntegerType)* 1000) as "price_range", dessertDF("*"))
priceRangeDessertDF.write.format("parquet").save("/output/price_range_dessert_parquet_nonpartitioned")
priceRangeDessertDF.write.format("parquet").partitionBy("price_range").save("/output/price_range_dessert_parquet_partitioned")

val nonPritionedDessertDF = spark.read.format("parquet").load("/output/price_range_dessert_parquet_nonpartitioned")
scala> nonPritionedDessertDF.where($"price_range" >= 5000).show
+-----------+------+-------------+-----+----+
|price_range|menuId|         name|price|kcal|
+-----------+------+-------------+-----+----+
|       5000|   D-1|  푸딩 파르페| 5300| 380|
|       5000|   D-2|  딸기 파르페| 5200| 320|
|       5000|   D-4|    치즈 무스| 5800| 288|
|       6000|   D-6|     티라미스| 6000| 251|
|       5000|  D-10|  크림 안미츠| 5000| 250|
|       6000|  D-11|고구마 파르페| 6500| 650|
+-----------+------+-------------+-----+----+

val pritionedDessertDF = spark.read.format("parquet").load("/output/price_range_dessert_parquet_partitioned")
scala> pritionedDessertDF.where($"price_range" >= 5000).show
+------+-------------+-----+----+-----------+
|menuId|         name|price|kcal|price_range|
+------+-------------+-----+----+-----------+
|   D-1|  푸딩 파르페| 5300| 380|       5000|
|   D-2|  딸기 파르페| 5200| 320|       5000|
|   D-4|    치즈 무스| 5800| 288|       5000|
|  D-10|  크림 안미츠| 5000| 250|       5000|
|   D-6|     티라미스| 6000| 251|       6000|
|  D-11|고구마 파르페| 6500| 650|       6000|
+------+-------------+-----+----+-----------+
```



#### 6.5

##### 3 .구조화된 데이터셋을 테이블로 다루기

```bash
spark-sql> create table dessert_tbl_json using org.apache.spark.sql.json options ( path "/output/dessert_json");
spark-sql> select name, price from dessert_tbl_json limit 3;
초콜릿 파르페	4900
푸딩 파르페	5300
딸기 파르페	5200
Time taken: 1.679 seconds, Fetched 3 row(s)

```



## 7장 스트림 데이터 처리하기 : 스파크 스트리밍

센서 등 기기에서 생성되는 수치 데이터나 웹 시스템의 사용자 접근 로그, 사용자의 구매 활동에 따른 재고 변동 데이터 와 같이 계속해서 생성되는 데이터를 `스트림 데이터`라 한다.

#### 7.5 스트리밍 동작 확인

- 넷캣

  넷캣(Netcat)은 TCP나 UDP 프로토콜을 사용하는 네트워크 연결에서 데이터를 읽고 쓰는 간단한 유틸리티 

  프로그램이다. nc는 network connection에 읽거나 쓴다. Network connection 에서 raw-data read, write를 

  할수 있는 유틸리티 프로그램으로 원하는 포트로 원하는 데이터를 주고받을수 있는 특징때문에 해킹에도 

  널리 이용되며, 컴퓨터 포렌식에 있어서 라이브시스템의 데이터를 손상없이 가져오기위해서도 사용될수 있습니다.

  

```bash
[TERM1] $ nc -l 9999

[TERM2] $ nc [ipaddress] 9999
hi
hello netcat!!!!
```

-> 클라이언트에서 입력한 모든 문자가 서버에 출력된다.
-> 간단한 채팅 서버를 만든 셈이다.
-> 클라이언트에서 <CTRL + D> 통해 끊을 때  종료 된다.

파일전송

클라이언트에서 명령의 결과를 바로 서버 측으로 전송하거나 파일을 전송할 수 있다.

클라이언트에서 넷캣 리스너로 파일을 전송할 수도 있고 역방향으로 파일을 전송할 수도 있다.

 ```bash
[TERM1] $ nc -l 9999 > /home/hadoop/listen.txt
[TERM2] $ ps auxf | nc [ipaddress] 9999
# or nc [ipaddress] < /tmp/input.txt
[TERM1] $ cat /tmp/listen.txt # 읽어 보면 파일 전송 되어있음
 ```



##### 10초 간격으로 텍스트를 읽고, 텍스트에 포함된 단어의 갯수 세기

```bash
nc -lk 9999 #terminal1 (데이터 입력하는 쪽)
/usr/local/spark/bin/spark-shell --master local[*] #terminal2 (데이터 받는 쪽)
```

```scala
import org.apache.spark.{SparkContext, SparkConf}
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.storage.StorageLevel
import org.apache.log4j.{Level, Logger}

Logger.getRootLogger.setLevel(Level.WARN)

val ssc = new StreamingContext(sc, Seconds(10))
val lines = ssc.socketTextStream("localhost", 9999, StorageLevel.MEMORY_AND_DISK_SER)
val words = lines.flatMap(_.split(" ")).filter(_.nonEmpty)
val wordCounts = words.map((_, 1)).reduceByKey(_ + _)
wordCounts.print()
ssc.start()
ssc.awaitTermination()
```

결과 > 

```bash
# 실시간 데이터 전송
hogehoge fuga hogehoge
fuga

# 데이터 받은 쪽 
-------------------------------------------                                     
Time: 1567432460000 ms
-------------------------------------------
(hogehoge,2)
(fuga,2)
```







#### 7.6 클러스터 환경에 애플리케이션 배포

```bash
#hadoop fs -mkdir  /data/sample_dir

import org.apache.spark.{SparkContext, SparkConf}
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.storage.StorageLevel
import org.apache.log4j.{Level, Logger}

Logger.getRootLogger.setLevel(Level.WARN)

val ssc = new StreamingContext(sc, Seconds(10))
val lines = ssc.textFileStream("/data/sample_dir/")
val words = lines.flatMap(_.split(" ")) 
val pairs = words.map((_, 1)) 
val wordCounts = pairs.reduceByKey(_ + _)
wordCounts.print()
ssc.start()
ssc.awaitTermination()

#hadoop fs -put  /usr/local/spark/README.md  /data/sample_dir/

```



#### 7.7 센서 데이터 스트림처리





### 8장 머신러닝 MLlib

과거 데이터의 정량적 관계를 파악함으로써 미래 데이터에 대한 예측을 하는 처리를 머신러닝이라 한다.



#### 8.2.1 MLlib 의 기본 요소

###### 데이터 타입

MLlib의 머신러닝 라이브러리는 벡터나 행렬 데이터 타입을 사용한다.

- 벡터(vector) : 1차원 데이터 타입
  - LocalVector - 기본적인 벡터 타입
  - LabeledPoint - LocalVector에 레이블을 부여한 데이터 타입

- 행렬(Matrix) : 1개 이상의 벡터로 구성되는 행렬 형식의 데이터 타입

  - LocalMatrix - 한 대의 호스트로 행렬 형식의 데이터를 다루기 위한 데이터 타입

  - DistributedMatrix - 대규모 데이터셋을 분산처리하기 위한 행렬 형식의 데이터 타입.

    복수의 RDD를 하나의 커다란 행렬 형식의 데이터셋으로 표현한다.

###### 통계용 기본 API

MLlib는 기본적인 통계 정보를 얻거나 동작을 확인할 수 있는 api 제공

- 통계정보 취득, 상관계수 계산, 층화 추출, 가설 검증, 랜덤 데이터 생성 API 등..



#### 8.2.2 MLlib에서 이용 가능한 머신러닝 알고리즘

- 분류와 회귀

  - 분류 - 미리 준비해둔 학습 데이터를 통해 분류 모델 작성 후, 처리 데이터에 모델을 적용하여 분류 할 수 있다. ( 선형 SVM, 로지스틱 회귀, 결정트리, 랜덤 포레스트, GBT 나이브 베이즈 )

    ex ) 스팸 메일 필터링

  - 회귀 - 준비해둔 학습 데이터로 예측 공식을 만들고 데이터를 입력하여 예상값 도출

    ( 선형회귀, 로지스틱 회귀, 결정 트리, 랜덤 포레스트, GBT, Isotonic 회귀 )

    ex ) 날씨,기온, 시간대 등을 기반으로 한 매출이나 이윤을 바탕으로 앞으로의 광고비를 합리적으로 산출

- 협업 필터링

   ex ) 상품에 대한 고객의 행동 이력 등의 정보를 바탕으로, 특정 고객이 선호하리라 예상 되는

  상품을을 상위 N개를 선택하여 추천한다. ( ALS )

- 클러스터링

  입력 데이터를 여러개의 부분집합으로 나눈다. ( k-means, 혼합 가우스 모델, PIC, LDA )

  ex ) 고객 특성을 바탕으로 고객을 특정 클러스터로 나누고, 그 클러스터에 적합한 내용으로 메일을 

  보내도록 활용한다.

- 차원 축소

  원래 데이터의 주요 정보를 어떤 형태로든 유지하면서도 데이터 차원을 축소 ( SVD, PCA )

  ex ) 고차원 데이터를 사람이 이해하기 쉬운 저차원 데이터로 변환, 차원이 너무 높아서 분석 알고리즘을 적용하기 어려운 경우

- 특징 추출/변환

  문서의 특징을 추출하는 방법 ( TF-IDF, Word2Vec ) 

  ex ) wordcount 를 벡터화 

- 빈발 패턴 마이닝

  FP-growth, 연관성 규칙 마이닝, 순차 패턴 마이닝

  ex ) 단팥빵을 산 사람은 우유를 사는 경우가 많다 와 같은 결론을 도출하여 상품 진열에 활용



#### 

#### 8.3 k-means 예제

```scala
import org.apache.spark.mllib.clustering.KMeans
import org.apache.spark.mllib.linalg.Vectors
val sparkHome = sys.env("SPARK_HOME")
val data = sc.textFile("file:///usr/local/spark/data/mllib/kmeans_data.txt")
val parseData = data.map{ s => Vectors.dense(s.split(' ').map(_.toDouble))}.cache()
val numClusters = 2
val numIterations = 20
val clusters = KMeans.train(parseData, numClusters, numIterations)
scala> clusters.k
scala> clusters.clusterCenters

val vec1 = Vectors.dense(0.3,0.3,0.3)
clusters.predict(vec1)
val vec2 = Vectors.dense(8.0,8.0,8.0)
clusters.predict(vec2)

scala> parseData.foreach(vec => println(vec + " => " + clusters.predict(vec)))
// val predictedLabels = parsedData.map(vector => clusters.predict(vector))
// predictedLabls.saveAsTextFile("output")
[0.0,0.0,0.0] => 0
[0.1,0.1,0.1] => 0
[0.2,0.2,0.2] => 0
[9.0,9.0,9.0] => 1
[9.1,9.1,9.1] => 1
[9.2,9.2,9.2] => 1
```



#### 8.4 Word2Vec으로 한국어 벡터화









































