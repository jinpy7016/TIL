#metastore 로 사용할 database 생성 및 metastore에 스키마 생성
[hadoop@master ~]$ su -
[root@master ~] mysql -u root -p
Enter password:
mysql> show databases;
mysql> CREATE DATABASE metastore_db;

mysql> USE metastore_db;
mysql> show tables;
mysql> SOURCE /usr/local/hive/scripts/metastore/upgrade/mysql/hive-schema-1.1.0.mysql.sql;
mysql> show tables;
 
# $HIVE_HOME/lib 아래 mysql-connector-java-5.1.36-bin.jar에 복사 
[hadoop@master ~]$ tar -xvf ./Downloads/mysql-connector-java-5.1.36.tar.gz
[hadoop@master ~]$ ls
[hadoop@master ~]$ cd  /home/hadoop/mysql-connector-java-5.1.36/
[hadoop@master ~]$ cp  mysql-connector-java-5.1.36-bin.jar /usr/local/hive/lib/


#하둡 시작
[hadoop@master ~]$ cd /usr/local/hadoop-2.7.7/sbin
[hadoop@master ~]$ ./start-all.sh

[hadoop@master ~]$ hive
hive> show databases;




hive> create database test_db;
hive> use test_db
hive> create table test ( name  varchar(10) );
hive> describe test

#하둡 DFS에 데이터베이스와 테이블은 디렉토리로 생성됨을 확인
[hadoop@master ~]$ hadoop fs -ls -R /user/

#metastore에서 생성한 데이터베이스와 테이블 메타 정보 확인
mysql> select OWNER, TBL_NAME, TBL_TYPE from TBLS;
mysql> select OWNER_NAME, OWNER_TYPE, NAME from DBS;




hive> drop database test_db cascade;
hive> show databases;

# hive에서 !시스템명령어 사용 가능 
ex) hive> !ls , !hadoop fs -ls /user/
# 종료 명령어
hive> exit; 

# csv 파일 db에 집어넣고 조회하기
hive> drop database test_db cascade;
hive> show databases;

hive> use default
hive> CREATE EXTERNAL TABLE airline (
Year string,
Month string,
DayofMonth string,
DayOfWeek string,
DepTime string,
CRSDepTime string,
ArrTime string,
CRSArrTime string,
UniqueCarrier string,
FlightNum string,
TailNum string,
ActualElapsedTime string,
CRSElapsedTime string,
AirTime string,
ArrDelay string,
DepDelay string,
Origin string,
Dest string,
Distance string,
TaxiIn string,
TaxiOut string,
Cancelled string,
CancellationCode string,
Diverted string,
CarrierDelay string,
WeatherDelay string,
NASDelay string,
SecurityDelay string,
LateAircraftDelay  string
)
ROW FORMAT DELIMITED
 FIELDS TERMINATED BY ',' 
 LINES TERMINATED BY '\n'
LOCATION '/data/airline/';

 > 월별 도착지연횟수를 출력하는 select문
hive> SELECT Year,Month, count(DepDelay)
      FROM airline
      GROUP BY Year,Month
      SORT BY Year,Month;   --reducer 별 처리 데이터 정렬, 전체 결과 정렬되지 않음


hive> SELECT Year,Month, count(DepDelay)
      FROM airline
      GROUP BY Year,Month
      ORDER BY Year,Month;   ----reducer개수 1개로 제한, 전체 정렬


hive> explain SELECT Year,Month, count(DepDelay)
      FROM airline
      GROUP BY Year,Month
      SORT BY Year,Month;

# dept.txt csv형태로 생성하여 hive로 조회
[hadoop@master ~]$ vi /home/hadoop/dept.txt 
10,'ACCOUNTING','NEW YORK'
20,'RESEARCH','DALLAS'
30,'SALES','CHICAGO'
40,'OPERATIONS','BOSTON'


hive> CREATE TABLE IF NOT EXISTS dept (
deptno INT, dname STRING, loc STRING)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';

hive> describe dept

hive> load data local inpath '/home/hadoop/dept.txt' 
      overwrite into table dept;
hive> select  * from dept;

hive> !hadoop fs -ls /user/hive/warehouse/

---------------------------------------------------------

# join
1. carriers.csv파일을 carriers테이블을 생성하고, 데이터 로딩하고
   hive> CREATE TABLE IF NOT EXISTS carriers (
   UniqueCarrier string,
   CarrierFullName String
   ) ROW FORMAT DELIMITED
 FIELDS TERMINATED BY ',' 
 LINES TERMINATED BY '\n'
LOCATION '/data/metadata/';
   
   hive> describe carriers;
   hive> select * from carriers limit 5;
   hive> !hadoop fs -ls /user/hive/warehouse/

2.  airlineinfo 테이블 생성
   hive> CREATE TABLE IF NOT EXISTS airlineinfo (
   UniqueCarrier string,
   CarrierFullName String,
   FlightNum string,
   TailNum string,
   Dest string,
   Distance string,
   Cancelled string
   );

hive> describe airlineinfo
hive> !hadoop fs -ls /user/hive/warehouse/

3. airline테이블과 carriers테이블의 조인 결과를 airlineinfo 테이블에 로딩

hive> INSERT  OVERWRITE  TABLE  airlineinfo 
 select  a.UniqueCarrier ,
   b.CarrierFullName ,
   a.FlightNum,
   a.TailNum ,
   a.Dest ,
   a.Distance ,
   a.Cancelled 
 from  airline a , carriers b  
where a.UniqueCarrier = substr(b.UniqueCarrier,2, 2);

hive> select * from airlineinfo limit 10;
hive> !hadoop fs -ls /user/hive/warehouse/

또는 

hive> drop table airlineinfo;
hive> CREATE TABLE airlineinfo 
 as
   select  a.UniqueCarrier  UniqueCarrier,
   b.CarrierFullName  CarrierFullName,
   a.FlightNum FlightNum,
   a.TailNum  TailNum ,
   a.Dest  Dest,
   a.Distance  Distance,
   a.Cancelled  Cancelled
 from  airline a , carriers b 
 where a.UniqueCarrier = substr(b.UniqueCarrier, 2, 2);

hive> select * from airlineinfo limit 10;
hive> !hadoop fs -ls /user/hive/warehouse/

hive> select count(*) from airlineinfo ;
hive> select count(*) from airline ;

결과 >
hive> select * from airlineinfo limit 10;
OK
WN	"Southwest Airlines Co."	335	N712SW	TPA	810	0
WN	"Wings Airways"			335	N712SW	TPA	810	0
WN	"Winair Inc."			335	N712SW	TPA	810	0
WN	"Southwest Airlines Co."	3231	N772SW	TPA	810	0
WN	"Wings Airways"			3231	N772SW	TPA	810	0
WN	"Winair Inc."			3231	N772SW	TPA	810	0
WN	"Southwest Airlines Co."	448	N428WN	BWI	515	0
WN	"Wings Airways"			448	N428WN	BWI	515	0
WN	"Winair Inc."			448	N428WN	BWI	515	0
WN	"Southwest Airlines Co."	1746	N612SW	BWI	515	0
Time taken: 0.11 seconds, Fetched: 10 row(s)

-----------------------------------------------------------------

# Hadoop R 연동
데이터 분석을 위한 통계 및 그래픽스를 지원하는 자유 소프트웨어 환경이다
컴퓨터 언어이자 다양한 패키지의 집합이다
함수기반

linux 설치법 >
1. CentOS와 Red Hat Enterprise Linux의 저장소에 포함되지 않은 패키지를 설치하기 위해 Fedora Extra Packages for Enterprise Linux (EPEL) repository를 설치
[root@master ~]# yum install epel-release
2. npm 설치
[root@master ~]# yum install npm
3. R 설치
[root@master ~]# yum install R 
4. 확인 
[root@master ~]# ls -l /usr/lib64
5. 소유자 hadoop으로 변경
[root@master ~]# chown -R hadoop:hadoop /usr/lib64/R
[root@master ~]# ls -l /usr/lib64
6. .bash_profile에 path추가
[hadoop@master ~]$ vi .bash_profile

export HADOOP_CMD=/usr/local/hadoop-2.7.7/bin/hadoop
export HADOOP_STREAMING=/usr/local/hadoop-2.7.7/share/hadoop/tools/lib/hadoop-streaming-2.7.7.jar
추가한 뒤 파일 실행
[hadoop@master ~]$ source ./.bash_profile 
7. R 실행 후 패키지 설치
[hadoop@master ~]$ R
필요한 패키지 설치
> install.packages(c("rJava", "Rcpp", "RJSONIO", "bitops", "digest", "functional", "stringr", "plyr", "reshape2", "caTools"))

https://github.com/RevolutionAnalytics/RHadoop/wiki 에서 패키지 5개 다운로드

>install.packages("/home/hadoop/Downloads/rhdfs_1.0.8.tar.gz", repos=NULL, type="source")
>install.packages("/home/hadoop/Downloads/rmr2_3.3.1.tar.gz", repos=NULL, type="source")
>install.packages("/home/hadoop/Downloads/plyrmr_0.6.0.tar.gz", repos=NULL, type="source")
>install.packages("/home/hadoop/Downloads/rhbase_1.2.1.tar.gz", repos=NULL, type="source")
>install.packages("/home/hadoop/Downloads/ravro_1.0.4.tar.gz", repos=NULL, type="source")
>install.packages(c("bit64", "rjson"))



# R 함수
[hadoop@master ~]$ vi test.R
print("R running~ from source")
a <- seq(1, 100 , by=2)
print(class(a))
print(a)

> source("/home/hadoop/test.R")
작업경로 설정
> setwd("/home/hadoop/")
> source("test.R")

[1] "R running~ from source"
[1] "numeric"
 [1]  1  3  5  7  9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49
[26] 51 53 55 57 59 61 63 65 67 69 71 73 75 77 79 81 83 85 87 89 91 93 95 97 99


> sum(1, NA, 2)  
[1] NA
> sum(1, NULL, 2)   
[1] 3
> sum(1, 2, NA, na.rm=T)  
[1] 3

RHADOOP 예제 
#1부터 1,000까지의 숫자를 생성/ 각 숫자 모두를 제곱하는 연산을 수행

library(rhdfs) # Rhadoop package for hdfs
hdfs.init()    # Start to connect HDFS, 반드시 rmr2를 로드하기 전
library(rmr2)  # RHadoop package for MapReduce
 
dfs.rmr("/tmp/ex1")
small.ints <- to.dfs(1:1000, "/tmp/ex1")
 
#from.dfs(small.ints)
	#$key
	#NULL
	#$val
	# [1]  1  2  3  4  5  6  7  8  9 10
 
result <- mapreduce(input = small.ints, 
	map = function(k,v) cbind(v,v^2)
)
out <- from.dfs(result)
print(out)

결과 >
[1,] ....
 . 
 .
 .

[994,]  994  988036
[995,]  995  990025
[996,]  996  992016
[997,]  997  994009
[998,]  998  996004
[999,]  999  998001
[1000,] 1000 1000000


# 자료값이 0보다 큰 수와 작은 수의 빈도계산

library(rhdfs) # Rhadoop package for hdfs
hdfs.init()    # Start to connect HDFS, 반드시 rmr2를 로드하기 전
library(rmr2)  # RHadoop package for MapReduce
 
random <- to.dfs(rnorm(100))
 
map <- function(k,v) {
 	key <- ifelse(v < 0, "less", "greater")
	keyval(key, 1)
}
reduce <- function(k,v) {
	keyval(k, length(v))
}
 
Freq <- mapreduce (
	input= random, output="/tmp/ex3",
	map = map, reduce = reduce
)
out <- from.dfs(Freq)
print(out)

결과 > 
$key
[1] "less"    "greater"

$val
[1] 52 48

# 문서자료에서 단어빈도계산
1. hdfs /tmp/ 에 readme.txt 생성
[hadoop@master ~]$ hadoop fs -put /usr/local/hadoop-2.7.7/README.txt /tmp/


[hadoop@master ~]$ vi wordcount_test.R 
library(rhdfs) # Rhadoop package for hdfs
hdfs.init()    # Start to connect HDFS, 반드시 rmr2를 로드하기 전
library(rmr2)  # RHadoop package for MapReduce
 
inputfile <- "/tmp/README.txt"
if(!hdfs.exists(inputfile)) stop("File is not found")
outputfile <- "/tmp/ex4"
if(hdfs.exists(outputfile)) hdfs.rm(outputfile)
 
map <- function(key, val){
	words.vec <- unlist(strsplit(val, split = " "))
	#lapply(words.vec, function(word) 
    keyval(words.vec, 1)
}
 
reduce <- function(word, counts ) {
	keyval(word, sum(counts))
}
 
result <- mapreduce(input = inputfile,
	output = outputfile, 
	input.format = "text", 
	map = map, 
	reduce = reduce, 
	combine = T
)
 
## wordcount output
freq.dfs <- from.dfs(result)
freq <- freq.dfs$val
word <- freq.dfs$key
oidx <- order(freq, decreasing=T)[1:10]
 
# Words frequency plot
barplot(freq[oidx], names.arg=word[oidx] )

> source("wordcount_test.R")


> 결과
barplot.jpg

----------------------------------------------------





