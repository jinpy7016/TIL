Spark?
인메모리 기반의 대용량 데이터 고속 처리 엔진으로 범용 분산 클러스터 컴퓨팅 프레임워크

Spark 구성 요소?
- 클러스터 매니저 : Spark standalone, Yarn, Mesos
- SparkCore 
- Spark SQL
- Spark Streaming - 실시간 처리
- MLlib
- Graph X

Spark에서는 데이터 처리하기 추상화된 모델 : RDD(복구가능한 분산 데이터셋)

SparkApplication 구현 단계 :
1. SparkContext 생성
   - Spark애플리케이션과 Spark 클러스터와의 연결을 담당하는 객체
   - 모든 스파크 애플리케이션은 SparkContext를 이용해 RDD나 accumulator 또는 broadcast 변수 등을 다루게 됩니다.
   - Spark 애플리케이션을 수행하는 데 필요한 각종 설정 정보를 담는 역할을 한다
2. RDD (불변데이터 모델, parition가능) 생성
   collection, HDFS, hive , CSV 등..
3. RDD 처리 - 변환연산(RDD의 요소의 구조 변경, filter처리, grouping...)    
4. 집계, 요약 처리 - Action연산 
5. 영속화 

SparkApplication => Job
Spark 클러스터 환경에서 node들  : SparkClient, Master노드, Worker노드
SparkClient 역할 - SparkApplication 배포하고 실행을 요청
Spark Master노드 역할 -  Spark 클러스터 환경에서 사용가능한 리소스들의 관리
Spark Worker노드 역할 - 할당받은 리소스(CPU core, memory)를 사용해서  SparkApplication 실행 
Spark Worker노드에서 실행되는 프로세스 - Executor는 RDD의 partition을 task단위로 실행

Spark장점
1. 반복처리와 연속으로 이루어지는 변환처리를 고속화 (메모리 기반)
2. 딥러닝, 머신러닝등의 실행환경에 적합한 환경 제공
3. 서로 다른 실행환경과 구조, 데이터들의 처리에 대해서 통합 환경 제공


sc.textFile() : file로 부터 RDD생성
collect
map, flatMap()
mkString("구분자")


=====================================================
wordcounttop3 예제

[hadoop@master ~]$ mkdir wordcounttop3-app
[hadoop@master ~]$ cd wordcounttop3-app/
[hadoop@master wordcounttop3-app]$ mkdir -p src/main/scala
[hadoop@master wordcounttop3-app]$ mkdir project
[hadoop@master wordcounttop3-app]$ mkdir -p src/main/scala/lab/spark/example
[hadoop@master wordcounttop3-app]$ cd src/main/scala/lab/spark/example/
[hadoop@master example]$ vi WordCountTop3.scala

package  lab.spark.example 
import org.apache.spark.{SparkConf, SparkContext}  
object WordCountTop3 {
  def main(args: Array[String]) {
   require(args.length >= 1,      
        "드라이버 프로그램의 인자에 단어를 세고자 하는 " 
        + "파일의 경로를 지정해 주세요")    
val conf = new SparkConf   
val sc = new SparkContext(conf)    
try {       
// 모든 단어에 대해 (단어, 등장횟수)형식의 튜플을 만든다       
val filePath = args(0)       
val wordAndCountRDD = sc.textFile(filePath) 
                   .flatMap(_.split("[ ,.]"))   
                   .filter(_.matches("""\p{Alnum}+""")) 
                   .map((_, 1))  
                   .reduceByKey(_ + _)  
        // 등장횟수가 가장 많은 단어 세개를 찾는다     
 val top3Words = wordAndCountRDD.map {
                    case (word, count) => (count, word)  
                 }
                .sortByKey(false)
                .map {         
                    case (count, word) => (word, count)       
                 }
                .take(3)     
 // 등장횟수가 가장 많은 단어 톱쓰리를 표준출력으로 표시한다  
     top3Words.foreach(println)   
} finally {     
sc.stop() 
 } 
} 
} 

[hadoop@master example]$ cd ~/wordcounttop3-app/
[hadoop@master wordcounttop3-app]$ vi build.sbt

name := "wordcounttop3-app"
version := "0.1"
scalaVersion := "2.11.12"
libraryDependencies ++= Seq("org.apache.spark" % "spark-core_2.11" % "2.4.3" % "provided")
assemblyOption in assembly := (assemblyOption in assembly).value.copy(includeScala = false)

[hadoop@master wordcounttop3-app]$ cd project/
[hadoop@master project]$ vi plugins.sbt

addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.14.10")

[hadoop@master project]$ cd ~/wordcounttop3-app
[hadoop@master wordcounttop3-app]$ sbt assembly
spark-submit --master local --class lab.spark.example.WordCountTop3 --name WordCountTop3 ~/wordcounttop3-app/target/scala-2.11/wordcounttop3-app-assembly-0.1.jar /data/spark/README.md

결과 > 
(the,24)
(Spark,17)
(to,17)

==========================================================
데이터 결합 처리 - join

-----------------------------
products.csv
0,송편(6개),12000
1,가래떡(3개),16000
2,연양갱,5000
3,호박엿(6개),16000
4,전병(20장),4000
5,별사탕,3200
6,백설기,3500
7,약과(5개),8300
8,강정(10개),15000
9,시루떡,6500
10,무지개떡,4300
11,깨강정(5개),14000
12,수정과(6컵),19000
13,절편(10개),15000
14,팥떡(8개),20000
15,생과자(10개),17000
16,식혜(2캔),21000
17,약식,4000
18,수수팥떡(6개),28000
19,팥죽(4개),16000
20,인절미(4개),10000
 
sales-october.csv
5830,2014-10-02 10:20:38,16,28
5831,2014-10-02 15:13:04,15,22
5832,2014-10-02 15:21:53,2,10
5833,2014-10-02 16:22:05,18,13
5834,2014-10-06 12:04:28,19,18
5835,2014-10-06 12:54:13,10,18
5836,2014-10-06 15:43:54,1,8
5837,2014-10-06 17:33:19,10,22
5838,2014-10-11 10:28:00,20,19
5839,2014-10-11 15:00:32,15,3
5840,2014-10-11 15:06:04,15,14
5841,2014-10-11 15:45:38,18,1
5842,2014-10-11 16:12:56,4,5
5843,2014-10-13 10:13:53,3,12
5844,2014-10-13 15:02:23,15,19
5845,2014-10-13 15:12:08,6,6
5846,2014-10-13 17:17:20,10,9
5847,2014-10-18 11:08:11,15,22
5848,2014-10-18 12:01:47,3,8
5849,2014-10-18 14:25:25,6,10
5850,2014-10-18 15:18:50,10,16
5851,2014-10-20 13:06:00,11,21
5852,2014-10-20 16:07:04,13,29
5853,2014-10-20 17:29:24,5,4
5854,2014-10-20 17:47:39,8,17
5855,2014-10-23 10:02:10,2,24
5836,2014-10-23 11:22:53,8,19
5857,2014-10-23 12:29:16,7,7
5858,2014-10-23 14:01:56,12,26
5859,2014-10-23 16:09:39,8,13
5860,2014-10-23 17:26:46,8,19

sales-november.csv
5861,2014-11-01 10:47:52,15,22
5863,2014-11-01 11:44:54,8,26
5864,2014-11-01 14:29:51,18,10
5865,2014-11-01 17:50:00,6,17
5867,2014-11-04 10:03:57,15,16
5868,2014-11-04 11:22:55,15,13
5869,2014-11-04 16:32:09,19,6
5870,2014-11-10 11:12:30,17,27
5871,2014-11-10 13:32:53,17,13
5872,2014-11-10 15:31:21,4,15
5873,2014-11-10 16:03:01,6,5
5874,2014-11-10 17:52:20,12,28
5875,2014-11-15 11:36:39,3,5
5876,2014-11-15 14:08:26,9,7
5877,2014-11-15 15:05:21,10,0
5878,2014-11-18 11:17:09,7,16
5879,2014-11-18 14:50:37,9,3
5880,2014-11-18 16:23:39,4,20
5881,2014-11-18 17:28:31,18,25
5882,2014-11-22 10:50:24,7,26
5883,2014-11-22 11:43:31,3,3
5884,2014-11-22 12:57:22,4,12
5885,2014-11-22 15:20:17,19,25
5886,2014-11-25 16:42:07,10,27
5887,2014-11-25 17:38:03,14,0
5888,2014-11-25 18:30:36,10,8
5889,2014-11-25 18:41:57,11,10
5890,2014-11-30 14:30:08,11,17
5862,2014-11-30 14:57:47,8,22
5866,2014-11-30 15:17:29,8,24

------------------------------------------
hdfs 에 파일 3개 업로드 후
scala-shell 에서 실행

def createSalesRDD(csvFile: String) = {
	val logRDD = sc.textFile(csvFile)
	logRDD.map { record =>
		val splitRecord = record.split(",")
		val productId = splitRecord(2)
		val numOfSold = splitRecord(3).toInt
		(productId, numOfSold)
	}
}

val salesOctRDD = createSalesRDD("/data/spark/sales-october.csv")
val salesNovRDD = createSalesRDD("/data/spark/sales-november.csv")

import org.apache.spark.rdd.RDD
def createOver50SoldRDD(rdd: RDD[(String, Int)]) ={
	rdd.reduceByKey(_ + _).filter(_._2 >= 50)
}

val octOver50SoldRDD = createOver50SoldRDD(salesOctRDD)
val novOver50SoldRDD = createOver50SoldRDD(salesNovRDD)
val bothOver50SoldRDD = octOver50SoldRDD.join(novOver50SoldRDD)
bothOver50SoldRDD.collect.foreach(println)

결과 >
(8,(68,72))                                                               
(15,(80,51))
























